{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.core import Dense, Activation, Flatten, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers import Dense, merge, Input, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils, plot_model\n",
    "from keras.models import Model, load_model, Sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import CSVLogger, Callback, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import csv\n",
    "\n",
    "from collections import Iterable\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import uniprot\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization learning rate\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Max length of protein\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "# number of channels per aa\n",
    "SEQ_NDIMS = 25 #X, B, Z\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "\n",
    "LATENT_REP_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots = pd.read_csv('../DeepPPI/dataset/Human/human_all_csv.tab', sep='\\t')[['protA', 'protB', 'Interaction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots_to_download = set(prots['protA']) | set(prots['protB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('uniprot_dict.bin'):\n",
    "    uniprot_data = pickle.load(open('uniprot_dict.bin', 'rb'))\n",
    "else:\n",
    "    uniprot_data = uniprot.batch_uniprot_metadata(prots_to_download, 'cache')\n",
    "    pickle.dump(uniprot_data, open('uniprot_dict.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f55adfdcc544f9fbca8fe0dc83bebf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=129971), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered = {k:[] for k in prots.columns.values}\n",
    "\n",
    "for idx, row in tqdm_notebook(prots.iterrows(), total=prots.shape[0]):\n",
    "    if row['protA'] in uniprot_data and row['protB'] in uniprot_data:\n",
    "        for k in filtered.keys():\n",
    "            filtered[k].append(row[k])\n",
    "filtered = pd.DataFrame(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots_names = list(uniprot_data.keys())\n",
    "for k in prots_names:\n",
    "    if len(uniprot_data[k]['sequence']) > MAX_LENGTH:\n",
    "        del uniprot_data[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "\n",
    "    def load(self, data):\n",
    "        self.inputs = self.process_data(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_one_hot(amino_acid):\n",
    "        one_hot = np.zeros(SEQ_NDIMS, dtype=np.int32)\n",
    "        one_hot[amino_acid] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def load_aa(self):\n",
    "        d = dict()\n",
    "        with open('aa_traits.tsv') as input:\n",
    "            input.readline()\n",
    "            index = 0\n",
    "            for line in input:\n",
    "                line = line.split()\n",
    "                d[line[0]] = self.convert_to_one_hot(index)\n",
    "                index += 1\n",
    "        return d\n",
    "\n",
    "    def process_single(self, protein, max_length=MAX_LENGTH):\n",
    "        aa = self.load_aa()\n",
    "        raw_protein = np.array(list(map(lambda x: aa[x], protein)), np.int32)\n",
    "\n",
    "        raw_protein = np.pad(raw_protein, ((0, max_length - raw_protein.shape[0]), (0, 0)),\n",
    "                             'constant', constant_values=0)\n",
    "        return raw_protein\n",
    "\n",
    "    def process_data(self, inputs):\n",
    "#         inputs = data['protein'].as_matrix()\n",
    "        inputs = np.array(list(map(self.process_single, inputs)))\n",
    "        return inputs\n",
    "\n",
    "    def split_data(self):\n",
    "        split_index = int(self.inputs.shape[0] * 0.1)\n",
    "        train = self.inputs[:-split_index]\n",
    "        test = self.inputs[-split_index:]\n",
    "        train, validation = train[:-split_index], train[-split_index:]\n",
    "\n",
    "        return train, validation, test\n",
    "\n",
    "\n",
    "class AutoEncoder:\n",
    "    def __init__(self):\n",
    "        self.input = Input(shape=(MAX_LENGTH, SEQ_NDIMS))\n",
    "\n",
    "        self.encoder = self.build_encoder(input=self.input)\n",
    "        self.decoder = self.build_decoder(self.encoder, LATENT_REP_SIZE)\n",
    "\n",
    "        self.autoencoder = Model(self.input, self.decoder)\n",
    "\n",
    "        self.autoencoder.compile(optimizer='Adam',\n",
    "                                 loss='categorical_crossentropy',\n",
    "                                 metrics=['accuracy'])\n",
    "\n",
    "    @staticmethod\n",
    "    def build_encoder(input):\n",
    "\n",
    "        enc = Convolution1D(filters=30, kernel_size=3)(input)\n",
    "        enc = Convolution1D(filters=5, kernel_size=1)(enc)\n",
    "\n",
    "        enc_lstm_for = LSTM(128, return_sequences=True, name='enc_lstm_for')(enc)\n",
    "        enc_lstm_back = LSTM(128, return_sequences=True, go_backwards=True, name='enc_lstm_back')(enc)\n",
    "        enc = merge([enc_lstm_for, enc_lstm_back], mode='concat')\n",
    "\n",
    "        enc = Flatten(name='flatten')(enc)\n",
    "        enc = Dense(units=LATENT_REP_SIZE, name='latent', activation='linear')(enc)\n",
    "\n",
    "        return enc\n",
    "\n",
    "    @staticmethod\n",
    "    def build_decoder(latent, latent_size=LATENT_REP_SIZE, max_length=MAX_LENGTH):\n",
    "\n",
    "        dec = RepeatVector(max_length, name='repeat_vector')(latent)\n",
    "        # dec = RepeatVector(max_length + 2, name='repeat_vector')(latent)\n",
    "        # dec = Convolution1D(filters=30, kernel_size=3)(dec)\n",
    "        # dec = Convolution1D(filters=5, kernel_size=1)(dec)\n",
    "\n",
    "        dec_lstm_for = LSTM(128, return_sequences=True, name='dec_lstm_for')(dec)\n",
    "        dec_lstm_back = LSTM(128, return_sequences=True, go_backwards=True, name='dec_lstm_back')(dec)\n",
    "        dec = merge([dec_lstm_for, dec_lstm_back], mode='concat')\n",
    "\n",
    "        return TimeDistributed(Dense(SEQ_NDIMS, activation='softmax'), name='decoded')(dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots_to_train = [uniprot_data[k]['sequence'] for k in uniprot_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.load(prots_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = data.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('logs.tsv', separator='\\t')\n",
    "checkpoints = ModelCheckpoint('checkpoints/epoch_{epoch:02d}.hdf5',\n",
    "                                 monitor='var_loss', verbose=1, save_best_only=False,\n",
    "                                 save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#     model.fit(train, train, BATCH_SIZE, NUM_EPOCHS, callbacks=[csv_logger, checkpoints],\n",
    "model.autoencoder.fit(train, train, BATCH_SIZE, NUM_EPOCHS, \n",
    "#                      callbacks=[csv_logger, checkpoints], # dafaq this shit doesnt work \n",
    "                     validation_split=0.1, validation_data=(validation, validation),\n",
    "                     shuffle=True, class_weight=None, verbose=1,\n",
    "                     sample_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
